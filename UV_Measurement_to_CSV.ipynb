{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjW3rQQ9Po1wznZikCrkYK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjanow/Masterarbeit/blob/main/UV_Measurement_to_CSV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hier werden die Rohdaten des BTS2048-UV-WP in eine nutzbare CSV-Datei geschrieben**"
      ],
      "metadata": {
        "id": "VbXcbReNyuop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dokumentenname: UV_Measurement_to_CSV.ipynb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Es werden die OR0-Daten (NasaAmes-Format), die eigentlich für den Versand an das BFS gedacht sind umgewandelt und in eine CSV-Datei geschrieben. Aufgrund der großen Datenmenge geschieht dies für jeden Monat getrennt."
      ],
      "metadata": {
        "id": "_Euu6x1XyxGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import der benötigten Module\n",
        "import os, sys\n",
        "import glob\n",
        "import json\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import re\n",
        "\n",
        "from scipy.io import netcdf\n",
        "from datetime import timedelta\n",
        "from datetime import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "6GGZwyBJzVsp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "month = '22.08'"
      ],
      "metadata": {
        "id": "D2EvtyQEfyu7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zu Beginn muss die Google-Drive eingerichtet werden, in der die Messdaten (OR0-Dateien) gespeichert sind. Danach werden alle verfügbaren Unterordner aufgerufen. So wird geprüft, ob der Mount richtig funktioniert hat."
      ],
      "metadata": {
        "id": "EKAQlrHyWWIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_path = '/content/drive/My Drive/Colab_Notebooks'\n",
        "# Durchsuche den Google Drive-Pfad\n",
        "for root, dirs, files in os.walk(drive_path):\n",
        "    for dir in dirs:\n",
        "        # Gib den Namen des Unterordners aus\n",
        "        print(os.path.join(root, dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_iN54-sVUVL",
        "outputId": "63431c84-3743-42ba-a630-fbccd1a70de1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Colab_Notebooks/CSV_Messdaten\n",
            "/content/drive/My Drive/Colab_Notebooks/CouchDB File\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten\n",
            "/content/drive/My Drive/Colab_Notebooks/CSV_UVI\n",
            "/content/drive/My Drive/Colab_Notebooks/CSV_Gewichtet\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/22.07\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/22.08\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/22.09\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/22.10\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/22.11\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/22.12\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.01\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.02\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.03\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.04\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.05\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.06\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.07\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.08\n",
            "/content/drive/My Drive/Colab_Notebooks/NasaAmes_Messdaten/Data/22.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Danach werden die einzelnen OR0-Dateien (NasaAmes Format) geladen und umgewandelt.\n",
        "\n",
        "Das NasaAmes Format: https://espoarchive.nasa.gov/content/Ames_Format_Specification_v20\n",
        "\n",
        "Die Messdaten sind unter dem FFI (File Format Index) 2005 gespeichert. Dieser Standard ist durch die Nasa nicht dokumentiert. Deshalb nachfolgend ein eigener Parser, der die Daten in eine nutzbare CSV umwandelt."
      ],
      "metadata": {
        "id": "ZRL9H_KEZpgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beschreibung der einzelnen Dictionaries und deren Inhalt:**\n",
        "\n",
        "- **file_names** = enthält die Dateinamen der einzelnen OR0-Dateien\n",
        "- **file_content** = Enthält den Inhalt der OR0-Dateien\n",
        "- **end_line_header** = enthält die Zeile an dem der Header endet"
      ],
      "metadata": {
        "id": "TI_gn6B6_qGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dateien einlesen:**"
      ],
      "metadata": {
        "id": "7_mIBaD2FwwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pfad zum Ordner mit den Dateien in Google Drive\n",
        "folder_path = '/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/' + month\n",
        "\n",
        "# OR0-Dateien im Ordner lesen\n",
        "file_paths = glob.glob(folder_path + '/*.OR0')\n",
        "\n",
        "# Liste für die Dateinamen erstellen\n",
        "file_names = []\n",
        "\n",
        "# Schleife über die Dateien\n",
        "for file_path in file_paths:\n",
        "\n",
        "    if os.path.getsize(file_path) > 100 * 1024:\n",
        "      # Dateiname extrahieren\n",
        "      file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "      # Datei öffnen und Inhalt lesen\n",
        "      with open(file_path, 'r') as file:\n",
        "          file_content = file.read()\n",
        "\n",
        "      # Variable für die Datei erstellen\n",
        "      globals()[file_name] = file_content\n",
        "\n",
        "      file_names.append(file_name)"
      ],
      "metadata": {
        "id": "LJXRx3MKM8L1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataframe mit Wellenlängen erstellen:**\n",
        "- Dieser wird später genutzt um die Spalten des Dataframe zu benennen."
      ],
      "metadata": {
        "id": "4sWE034DaFBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_df_Wellenlaenge(start, end, step):\n",
        "    # Erstelle eine Liste mit den gewünschten Werten\n",
        "    numbers_list = [round(num, 3) for num in list(np.arange(start, end + step, step))]\n",
        "    # Erstelle den Dataframe\n",
        "    df = pd.DataFrame({'Wellenlaenge': numbers_list})\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "vR4lB-9hAJZ6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_Wellenlaenge = np.round(np.arange(290.0, 420.05, 0.1), decimals = 1)\n",
        "df_Wellenlaenge = pd.DataFrame({'Wellenlaenge': np_Wellenlaenge})"
      ],
      "metadata": {
        "id": "ke6b38SlA3B1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**String aufteilen in einzelne Zeile schreiben:**\n",
        "\n",
        "- Zur weitern Verarbeitung müssen alle Elemente als einzelene Strings abgespeichet werden."
      ],
      "metadata": {
        "id": "P4FUkD2Lua3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_content = {}  # Dictionary für die Messungen erstellen\n",
        "\n",
        "for file_name in file_names:\n",
        "    file_variables = globals()[file_name]\n",
        "    file_content[file_name] = file_variables\n",
        "\n",
        "# Auf Variablen zugreifen und String in Zeilen aufteilen\n",
        "for file_name, variable in file_content.items():\n",
        "    file_content[file_name] = file_content[file_name].split('\\n')"
      ],
      "metadata": {
        "id": "JxBOhrPFQIYc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# die ersten 10 File Namen ausgeben und die ersten 20 Zeilen der ersten Messung\n",
        "# print(file_names[:10])\n",
        "# file_content['SA220615'][:20]"
      ],
      "metadata": {
        "id": "39IUSb11vuGi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Header extrahieren:**"
      ],
      "metadata": {
        "id": "AaUuqtlgK2Qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion um den Dateiheader zu extrahieren\n",
        "def perform_action(file_variables, file_names):\n",
        "    header_dict = {file_name: \"\" for file_name in file_names}\n",
        "    end_line_header_fnc = 0\n",
        "\n",
        "    for file_name, data in file_variables.items():\n",
        "        for i, line in enumerate(data):\n",
        "            header_dict[file_name] += line + \"\\n\"\n",
        "            if line.strip() == \"Pyranometer: readout interval [secs]=5\":\n",
        "                end_line_header_fnc = i\n",
        "                break\n",
        "\n",
        "    return header_dict, end_line_header_fnc"
      ],
      "metadata": {
        "id": "rBjd_Y6lEw80"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_header, end_line_header = perform_action(file_content, file_names)"
      ],
      "metadata": {
        "id": "nUdv6oMbE2-q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Header aus Datensatz löschen:**"
      ],
      "metadata": {
        "id": "L_0hAYmkOyxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_header(lines_content, end_line):\n",
        "    lines_WO_header_fnc = {}\n",
        "    lines_WO_header_fnc = lines_content.copy()\n",
        "\n",
        "    for key, value in lines_WO_header_fnc.items():\n",
        "        del value[:end_line+1]\n",
        "\n",
        "    return lines_WO_header_fnc"
      ],
      "metadata": {
        "id": "a9HPMKEdffX1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_WO_header = remove_header(file_content, end_line_header)"
      ],
      "metadata": {
        "id": "hxWLzYdkff7w"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(lines_WO_header['SA220615'][:20])"
      ],
      "metadata": {
        "id": "fOR-FkULhDAQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Elemente aufteilen:**"
      ],
      "metadata": {
        "id": "glLu0-yiZ724"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_and_split(input_list):\n",
        "    result = []\n",
        "    for sublist in input_list:\n",
        "        elements = sublist.split()\n",
        "        result.extend(elements)\n",
        "    return result"
      ],
      "metadata": {
        "id": "scwLZbQNt1pO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dict(input_dict):\n",
        "    processed_dict = {}\n",
        "    for key, value in input_dict.items():\n",
        "        processed_value = flatten_and_split(value)\n",
        "        sublists = []\n",
        "        sublist = []\n",
        "        for element in processed_value:\n",
        "            if element.isdigit() and len(element) == 5:\n",
        "                if sublist:\n",
        "                    sublists.append(sublist)\n",
        "                    sublist = []\n",
        "            sublist.append(element)\n",
        "        if sublist:\n",
        "            sublists.append(sublist)\n",
        "        processed_dict[key] = sublists\n",
        "    return processed_dict"
      ],
      "metadata": {
        "id": "aqXTQ0E3t1zT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_dict = process_dict(lines_WO_header)"
      ],
      "metadata": {
        "id": "lTaf7gzyt32j"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(processed_dict['SA220615'][:2])"
      ],
      "metadata": {
        "id": "hZZFLjAxt6hC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zeitstempel in Datensatz finden:**"
      ],
      "metadata": {
        "id": "uscoR4cRFvIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_5_digit_integers(input_list):\n",
        "    result = []\n",
        "    for sublist in input_list:\n",
        "        for element in sublist:\n",
        "            if isinstance(element, str) and element.isdigit() and len(element) == 5:\n",
        "                result.append(element)\n",
        "    return result"
      ],
      "metadata": {
        "id": "WI-aukhcgE1E"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_5_digit_integers_in_dict(input_dict):\n",
        "    result_dict = {}\n",
        "    for key, value in input_dict.items():\n",
        "        result_dict[key] = find_5_digit_integers(value)\n",
        "    return result_dict"
      ],
      "metadata": {
        "id": "gvgssDUayRve"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict = find_5_digit_integers_in_dict(processed_dict)"
      ],
      "metadata": {
        "id": "3dJYofTuiM6m"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dict in Dataframe speichern:**"
      ],
      "metadata": {
        "id": "gapz7ewHV3F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstelle eine leere Liste, um die Zeilen für den DataFrame aufzunehmen\n",
        "data_rows = []\n",
        "df_Messdaten = []\n",
        "\n",
        "\n",
        "# Iteriere durch das verschachtelte Dictionary und erstelle Zeilen für den DataFrame\n",
        "for key, value in processed_dict.items():\n",
        "    for sublist in value:\n",
        "        data_rows.append([key] + sublist)\n",
        "\n",
        "# Definiere Spaltennamen für den DataFrame\n",
        "columns = ['Datum'] + [f'Wert{i}' for i in range(1, len(data_rows[0]))]\n",
        "\n",
        "# Erstelle den Pandas DataFrame\n",
        "df_Messdaten = pd.DataFrame(data_rows, columns=columns)"
      ],
      "metadata": {
        "id": "IxMzzikpWGXY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bestimme die Indexposition, ab der die neuen Spaltennamen zugewiesen werden sollen\n",
        "start_index = 18  # Beispiel: Beginne ab der dritten Spalte\n",
        "\n",
        "# Extrahiere die neuen Spaltennamen aus dem zweiten DataFrame\n",
        "new_column_names = df_Wellenlaenge['Wellenlaenge'].tolist()\n",
        "\n",
        "# Ändere die Spaltennamen des DataFrames ab der angegebenen Indexposition\n",
        "for i, new_name in enumerate(new_column_names):\n",
        "    df_Messdaten.columns.values[start_index + i] = new_name"
      ],
      "metadata": {
        "id": "HK9mS7hkjQGB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion zum Konvertieren des Datumsformats\n",
        "def convert_date_format(date_str):\n",
        "    date_str = date_str[2:]  # Entferne das \"SA\"-Präfix\n",
        "    date_obj = datetime.strptime(date_str, \"%y%m%d\")\n",
        "    return date_obj"
      ],
      "metadata": {
        "id": "QMcU_jOo6MlD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wende die Funktion auf die Spalte an\n",
        "df_Messdaten['Datum'] = df_Messdaten['Datum'].apply(convert_date_format)"
      ],
      "metadata": {
        "id": "3ymirsQx747Z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spalten_zum_loeschen = df_Messdaten.columns[2:18]  # Index 2 bis Index 20 (Python verwendet 0-basierte Indizes)\n",
        "\n",
        "df_Messdaten.drop(spalten_zum_loeschen, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "wqjrgDVjk70T"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splaten umbennen und ordnen\n",
        "\n",
        "spalte_uhrzeit = pd.to_datetime(df_Messdaten['Wert1'], unit='s').dt.time\n",
        "\n",
        "position = 1\n",
        "df_Messdaten.insert(position, 'Uhrzeit', spalte_uhrzeit)\n",
        "\n",
        "spaltenname = 'Messzeitpunkt'\n",
        "df_Messdaten.rename(columns={'Wert1': spaltenname}, inplace=True)\n",
        "\n",
        "df_Messdaten.insert(0, 'Datetime', df_Messdaten['Datum'] + pd.to_timedelta(df_Messdaten['Uhrzeit'].astype(str)))"
      ],
      "metadata": {
        "id": "XWd1u0bkm4Y8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataframe_to_drive(dataframe, folder_path, filename):\n",
        "\n",
        "    # Erstelle den vollen Pfad zur Datei\n",
        "    full_path = os.path.join(folder_path, filename)\n",
        "\n",
        "    # Speichere den DataFrame als CSV-Datei auf Google Drive\n",
        "    dataframe.to_csv(full_path, index=False)\n",
        "\n",
        "    print(f'Der DataFrame wurde als {filename} in {folder_path} auf Google Drive gespeichert.')"
      ],
      "metadata": {
        "id": "w8FRjN2a0pic"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dataframe_to_drive(df_Messdaten, '/content/drive/My Drive/Colab_Notebooks/CSV_Messdaten', month)"
      ],
      "metadata": {
        "id": "DO8lF9Vxz6L_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}