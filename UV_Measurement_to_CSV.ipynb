{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZFKoozg/NV989jwjS3DRD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjanow/Masterarbeit/blob/main/UV_Measurement_to_CSV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hier werden die Rohdaten des BTS2048-UV-WP in eine nutzbare CSV-Datei geschrieben"
      ],
      "metadata": {
        "id": "VbXcbReNyuop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dokumentenname: UV_Measurement_to_CSV.ipynb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Es werden die OR0-Daten (NasaAmes-Format), die eigentlich für den Versand an das BFS gedacht sind umgewandelt und in eine CSV-Datei geschrieben. Aufgrund der großen Datenmenge geschieht dies für jeden Monat getrennt."
      ],
      "metadata": {
        "id": "_Euu6x1XyxGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import der benötigten Module\n",
        "import os, sys\n",
        "import glob\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.io import netcdf\n",
        "from datetime import timedelta\n",
        "from datetime import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "6GGZwyBJzVsp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Monat der Exportiert werden soll:**"
      ],
      "metadata": {
        "id": "HYdn1p8-PgJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "month = '23.06'"
      ],
      "metadata": {
        "id": "b_lzKoe_Pfgd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zu Beginn muss die Google-Drive eingerichtet werden, in der die Messdaten (OR0-Dateien) gespeichert sind. Danach werden alle verfügbaren Unterordner aufgerufen. So wird geprüft, ob der Mount richtig funktioniert hat."
      ],
      "metadata": {
        "id": "EKAQlrHyWWIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "# Durchsuche den Google Drive-Pfad\n",
        "for root, dirs, files in os.walk(drive_path):\n",
        "    for dir in dirs:\n",
        "        # Gib den Namen des Unterordners aus\n",
        "        print(os.path.join(root, dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_iN54-sVUVL",
        "outputId": "29612049-9ed4-4376-81b1-fc82291130ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab_Notebooks\n",
            "/content/drive/MyDrive/Colab Notebooks\n",
            "/content/drive/MyDrive/Colab_Notebooks/CSV_Messdaten\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten\n",
            "/content/drive/MyDrive/Colab_Notebooks/CSV_UVI\n",
            "/content/drive/MyDrive/Colab_Notebooks/CSV_Gewichtet\n",
            "/content/drive/MyDrive/Colab_Notebooks/CSV_SolarRadiation\n",
            "/content/drive/MyDrive/Colab_Notebooks/netCDF4_Wetterdaten\n",
            "/content/drive/MyDrive/Colab_Notebooks/CAMS_Vorhersage\n",
            "/content/drive/MyDrive/Colab_Notebooks/Clean_Data\n",
            "/content/drive/MyDrive/Colab_Notebooks/plot_daily_UVI\n",
            "/content/drive/MyDrive/Colab_Notebooks/SOLYS_Messdaten\n",
            "/content/drive/MyDrive/Colab_Notebooks/CSV_Zwischenspeicher\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/22.07\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/22.08\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/22.09\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/22.10\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/22.11\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/22.12\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.01\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.02\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.03\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.04\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.05\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.06\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.07\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/23.08\n",
            "/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/22.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Danach werden die einzelnen OR0-Dateien (NasaAmes Format) geladen und umgewandelt.\n",
        "\n",
        "Das NasaAmes Format: https://espoarchive.nasa.gov/content/Ames_Format_Specification_v20\n",
        "\n",
        "Die Messdaten sind unter dem FFI (File Format Index) 2005 gespeichert. Dieser Standard ist durch die Nasa nicht dokumentiert. Deshalb nachfolgend ein eigener Parser, der die Daten in eine nutzbare CSV umwandelt."
      ],
      "metadata": {
        "id": "ZRL9H_KEZpgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beschreibung der einzelnen Dictionaries und deren Inhalt:**\n",
        "\n",
        "- **file_names** = enthält die Dateinamen der einzelnen OR0-Dateien\n",
        "- **file_content** = Enthält den Inhalt der OR0-Dateien\n",
        "- **end_line_header** = enthält die Zeile an dem der Header endet"
      ],
      "metadata": {
        "id": "TI_gn6B6_qGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dateien einlesen:"
      ],
      "metadata": {
        "id": "7_mIBaD2FwwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pfad zum Ordner mit den Dateien in Google Drive\n",
        "folder_path = '/content/drive/MyDrive/Colab_Notebooks/NasaAmes_Messdaten/Data/' + month\n",
        "\n",
        "# OR0-Dateien im Ordner lesen\n",
        "file_paths = glob.glob(folder_path + '/*.OR0')\n",
        "\n",
        "# Liste für die Dateinamen erstellen\n",
        "file_names = []\n",
        "\n",
        "# Schleife über die Dateien\n",
        "for file_path in file_paths:\n",
        "\n",
        "    if os.path.getsize(file_path) > 100 * 1024:\n",
        "      # Dateiname extrahieren\n",
        "      file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "      # Datei öffnen und Inhalt lesen\n",
        "      with open(file_path, 'r') as file:\n",
        "          file_content = file.read()\n",
        "\n",
        "      # Variable für die Datei erstellen\n",
        "      globals()[file_name] = file_content\n",
        "\n",
        "      file_names.append(file_name)"
      ],
      "metadata": {
        "id": "LJXRx3MKM8L1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataframe mit Wellenlängen erstellen:**\n",
        "- Dieser wird später genutzt um die Spalten des Dataframe zu benennen."
      ],
      "metadata": {
        "id": "4sWE034DaFBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_df_Wellenlaenge(start, end, step):\n",
        "    # Erstelle eine Liste mit den gewünschten Werten\n",
        "    numbers_list = [round(num, 3) for num in list(np.arange(start, end + step, step))]\n",
        "    # Erstelle den Dataframe\n",
        "    df = pd.DataFrame({'Wellenlaenge': numbers_list})\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "vR4lB-9hAJZ6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_Wellenlaenge = np.round(np.arange(290.0, 420.05, 0.1), decimals = 1)\n",
        "df_Wellenlaenge = pd.DataFrame({'Wellenlaenge': np_Wellenlaenge})"
      ],
      "metadata": {
        "id": "ke6b38SlA3B1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**String aufteilen in einzelne Zeile schreiben:**\n",
        "\n",
        "- Zur weitern Verarbeitung müssen alle Elemente als einzelene Strings abgespeichet werden."
      ],
      "metadata": {
        "id": "P4FUkD2Lua3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_content = {}  # Dictionary für die Messungen erstellen\n",
        "\n",
        "for file_name in file_names:\n",
        "    file_variables = globals()[file_name]\n",
        "    file_content[file_name] = file_variables\n",
        "\n",
        "# Auf Variablen zugreifen und String in Zeilen aufteilen\n",
        "for file_name, variable in file_content.items():\n",
        "    file_content[file_name] = file_content[file_name].split('\\n')"
      ],
      "metadata": {
        "id": "JxBOhrPFQIYc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Header extrahieren:**"
      ],
      "metadata": {
        "id": "AaUuqtlgK2Qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion um den Dateiheader zu extrahieren\n",
        "def perform_action(file_variables, file_names):\n",
        "    header_dict = {file_name: \"\" for file_name in file_names}\n",
        "    end_line_header_fnc = 0\n",
        "\n",
        "    for file_name, data in file_variables.items():\n",
        "        for i, line in enumerate(data):\n",
        "            header_dict[file_name] += line + \"\\n\"\n",
        "            if line.strip() == \"Pyranometer: readout interval [secs]=5\":\n",
        "                end_line_header_fnc = i\n",
        "                break\n",
        "\n",
        "    return header_dict, end_line_header_fnc"
      ],
      "metadata": {
        "id": "rBjd_Y6lEw80"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_header, end_line_header = perform_action(file_content, file_names)"
      ],
      "metadata": {
        "id": "nUdv6oMbE2-q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Header aus Datensatz löschen:**"
      ],
      "metadata": {
        "id": "L_0hAYmkOyxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_header(lines_content, end_line):\n",
        "    lines_WO_header_fnc = {}\n",
        "    lines_WO_header_fnc = lines_content.copy()\n",
        "\n",
        "    for key, value in lines_WO_header_fnc.items():\n",
        "        del value[:end_line+1]\n",
        "\n",
        "    return lines_WO_header_fnc"
      ],
      "metadata": {
        "id": "a9HPMKEdffX1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_WO_header = remove_header(file_content, end_line_header)"
      ],
      "metadata": {
        "id": "hxWLzYdkff7w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Elemente aufteilen:"
      ],
      "metadata": {
        "id": "glLu0-yiZ724"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_and_split(input_list):\n",
        "    result = []\n",
        "    for sublist in input_list:\n",
        "        elements = sublist.split()\n",
        "        result.extend(elements)\n",
        "    return result"
      ],
      "metadata": {
        "id": "scwLZbQNt1pO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dict(input_dict):\n",
        "    processed_dict = {}\n",
        "    for key, value in input_dict.items():\n",
        "        processed_value = flatten_and_split(value)\n",
        "        sublists = []\n",
        "        sublist = []\n",
        "        for element in processed_value:\n",
        "            if element.isdigit() and len(element) == 5:\n",
        "                if sublist:\n",
        "                    sublists.append(sublist)\n",
        "                    sublist = []\n",
        "            sublist.append(element)\n",
        "        if sublist:\n",
        "            sublists.append(sublist)\n",
        "        processed_dict[key] = sublists\n",
        "    return processed_dict"
      ],
      "metadata": {
        "id": "aqXTQ0E3t1zT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_dict = process_dict(lines_WO_header)"
      ],
      "metadata": {
        "id": "lTaf7gzyt32j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zeitstempel in Datensatz finden:**"
      ],
      "metadata": {
        "id": "uscoR4cRFvIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion zum finden eines 5 stelligen Integers (= Zeitstempel)\n",
        "def find_5_digit_integers(input_list):\n",
        "    result = []\n",
        "    for sublist in input_list:\n",
        "        for element in sublist:\n",
        "            if isinstance(element, str) and element.isdigit() and len(element) == 5:\n",
        "                result.append(element)\n",
        "    return result"
      ],
      "metadata": {
        "id": "WI-aukhcgE1E"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion, die durch das dict iterriert\n",
        "def find_5_digit_integers_in_dict(input_dict):\n",
        "    result_dict = {}\n",
        "    for key, value in input_dict.items():\n",
        "        result_dict[key] = find_5_digit_integers(value)\n",
        "    return result_dict"
      ],
      "metadata": {
        "id": "gvgssDUayRve"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict = find_5_digit_integers_in_dict(processed_dict)"
      ],
      "metadata": {
        "id": "3dJYofTuiM6m"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dict in Dataframe speichern:**"
      ],
      "metadata": {
        "id": "gapz7ewHV3F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstelle eine leere Liste, um die Zeilen für den DataFrame aufzunehmen\n",
        "data_rows = []\n",
        "df_rows = pd.DataFrame()\n",
        "df_Messdaten = pd.DataFrame()\n",
        "\n",
        "# Iteriere durch das verschachtelte Dictionary und erstelle Zeilen für den DataFrame\n",
        "for key, value in processed_dict.items():\n",
        "    for sublist in value:\n",
        "        data_rows.append([key] + sublist)\n",
        "\n",
        "# Definiere Spaltennamen für den DataFrame\n",
        "columns = ['Datum'] + [f'Wert{i}' for i in range(1, len(data_rows[0]))]\n",
        "\n",
        "# Erstelle den Pandas DataFrame\n",
        "df_rows = pd.DataFrame(data_rows, columns=columns)\n",
        "df_Messdaten = df_rows.copy()"
      ],
      "metadata": {
        "id": "IxMzzikpWGXY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "ef7ff354-c91e-4af0-dae8-4bce7bd26283"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f142c665c69e>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Definiere Spaltennamen für den DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Datum'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf'Wert{i}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Erstelle den Pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prüfen, ob der Inhalt des DF korrekt ist:**"
      ],
      "metadata": {
        "id": "otw05cgGuEG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_rows"
      ],
      "metadata": {
        "id": "A9285K3xuDk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Konvertieren des DF:"
      ],
      "metadata": {
        "id": "hEUUcL5RuL_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bestimme die Indexposition, ab der die neuen Spaltennamen zugewiesen werden sollen\n",
        "start_index = 18  # Beginne ab der 18ten Spalte um an die Messdaten zu kommen\n",
        "\n",
        "new_column_names = []\n",
        "\n",
        "# Extrahiere die neuen Spaltennamen aus dem zweiten DataFrame\n",
        "new_column_names = df_Wellenlaenge['Wellenlaenge'].tolist()\n",
        "print(new_column_names)\n",
        "\n",
        "# Ändere die Spaltennamen des DataFrames ab der angegebenen Indexposition\n",
        "for i, new_name in enumerate(new_column_names):\n",
        "    df_Messdaten.columns.values[start_index + i] = new_name"
      ],
      "metadata": {
        "id": "HK9mS7hkjQGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Messdaten"
      ],
      "metadata": {
        "id": "FYgIYarXhCIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splaten in float umwandeln\n",
        "for spalte in new_column_names:\n",
        "    # Konvertiere nur, wenn die Spalte im DataFrame existiert\n",
        "    if str(spalte) in df_Messdaten.columns:\n",
        "        df_Messdaten[str(spalte)] = df_Messdaten[str(spalte)].astype(float)"
      ],
      "metadata": {
        "id": "mIi1pCXGsjJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion zum Konvertieren des Datumsformats\n",
        "def convert_date_format(date_str):\n",
        "    date_str = date_str[2:]  # Entferne das \"SA\"-Präfix\n",
        "    date_obj = datetime.strptime(date_str, \"%y%m%d\")\n",
        "    return date_obj"
      ],
      "metadata": {
        "id": "QMcU_jOo6MlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wende die Funktion auf die Spalte an\n",
        "df_Messdaten['Datum'] = df_Messdaten['Datum'].apply(convert_date_format)"
      ],
      "metadata": {
        "id": "3ymirsQx747Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DF muss exportiert und wieder importiert werden, damit die Zeilen ordnungsgemäß gelöscht werden können."
      ],
      "metadata": {
        "id": "RH5chIOBPAz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataframe_to_drive(dataframe, folder_path, filename):\n",
        "\n",
        "    # Erstelle den vollen Pfad zur Datei\n",
        "    full_path = os.path.join(folder_path, filename)\n",
        "\n",
        "    # Speichere den DataFrame als CSV-Datei auf Google Drive\n",
        "    dataframe.to_csv(full_path, index=False)\n",
        "\n",
        "    print(f'Der DataFrame wurde als {filename} in {folder_path} auf Google Drive gespeichert.')"
      ],
      "metadata": {
        "id": "w8FRjN2a0pic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe abspeichern und wieder importieren\n",
        "folder_path2 = '/content/drive/My Drive/Colab_Notebooks/CSV_Zwischenspeicher/'\n",
        "save_dataframe_to_drive(df_Messdaten, folder_path2, month)"
      ],
      "metadata": {
        "id": "J8HZF6_0l9Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Messdaten_drop = pd.DataFrame()\n",
        "df_drop = pd.DataFrame()\n",
        "df_drop = pd.read_csv(folder_path2 + month)"
      ],
      "metadata": {
        "id": "jJEYuId5m1CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spalten_zum_loeschen = df_drop.columns[2:18]  # Index 3 bis Index 19\n",
        "df_Messdaten_drop = df_drop.drop(spalten_zum_loeschen, axis=1).copy()"
      ],
      "metadata": {
        "id": "wqjrgDVjk70T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Umbenennen der Spalte mit dem Zeitstempel\n",
        "df_Messdaten_drop.rename(columns={'Wert1': 'Messzeitpunkt'}, inplace=True)"
      ],
      "metadata": {
        "id": "6_sh7TVXcPqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Messdaten_drop"
      ],
      "metadata": {
        "id": "2zsIqOGrk-K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df_Messdaten.isnull().sum(), df_Messdaten_drop.isnull().sum())"
      ],
      "metadata": {
        "id": "tbYaaFvtpQBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export und Import"
      ],
      "metadata": {
        "id": "hNL9z9a0N1Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Messdaten_drop.columns"
      ],
      "metadata": {
        "id": "H50uAN6ooqTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seconds_to_time(seconds):\n",
        "    # Sicherstellen, dass die Eingabe eine Ganzzahl ist\n",
        "    if not isinstance(seconds, int):\n",
        "        raise ValueError(\"Die Eingabe muss eine Ganzzahl sein\")\n",
        "\n",
        "    # Sicherstellen, dass die Eingabe im Bereich eines Tages liegt\n",
        "    if seconds < 0 or seconds >= 24*60*60:\n",
        "        raise ValueError(\"Die Eingabe muss zwischen 0 und 86399 Sekunden liegen\")\n",
        "\n",
        "    # Erstellen eines datetime.time-Objekts aus den Sekunden\n",
        "    return (datetime.min + timedelta(seconds=seconds)).time()"
      ],
      "metadata": {
        "id": "Qo3UqmUGbLJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Messdaten_drop.insert(1, 'Uhrzeit', df_Messdaten_drop['Messzeitpunkt'].apply(seconds_to_time))"
      ],
      "metadata": {
        "id": "F9An-Y14bDzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_Messdaten_drop"
      ],
      "metadata": {
        "id": "I5szUUFEbNs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die Splate Uhrzeit gibt es noch nicht und muss noch eingefügt werden."
      ],
      "metadata": {
        "id": "_OWBjwQaKsN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splaten umbennen und ordnen\n",
        "spalte_uhrzeit = pd.to_datetime(df_Messdaten_drop['Wert1'], unit='s').dt.time\n",
        "\n",
        "print(spalte_uhrzeit)\n",
        "\n",
        "position = 1\n",
        "df_Messdaten_drop.insert(position, 'Uhrzeit', spalte_uhrzeit)\n",
        "\n",
        "spaltenname = 'Messzeitpunkt'\n",
        "df_Messdaten_drop.rename(columns={'Wert1': spaltenname}, inplace=True)"
      ],
      "metadata": {
        "id": "RUMO_4LRKVfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datetimespalte Einfügen\n",
        "df_Messdaten_drop['Datum'] = pd.to_datetime(df_Messdaten_drop['Datum'], format=\"%Y-%m-%d\").dt.date\n",
        "\n",
        "df_Messdaten_drop['Datetime'] = df_Messdaten_drop.apply(lambda row: datetime.combine(row['Datum'], row['Uhrzeit']), axis=1)\n",
        "\n",
        "df_Messdaten_drop.insert(0, 'Datetime', df_Messdaten_drop.pop('Datetime'))"
      ],
      "metadata": {
        "id": "XWd1u0bkm4Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Messdaten_drop"
      ],
      "metadata": {
        "id": "U4i77KdFzcVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Allgemeine Informationen über den Dataframe:**"
      ],
      "metadata": {
        "id": "yBJ_HtttaxJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def finde_max_werte_fuer_alle_spalten(df):\n",
        "    # Erstellt ein Dictionary, um die maximalen Werte jeder Spalte zu speichern\n",
        "    max_werte = {}\n",
        "    id_werte = {}\n",
        "    # Iteriere über alle Spalten im DataFrame\n",
        "    for spalte in df.columns:\n",
        "        # Ignoriere nicht-numerische Spalten\n",
        "        if pd.api.types.is_numeric_dtype(df[spalte]):\n",
        "            max_werte[spalte] = df[spalte].max()      # Maximaler Wert\n",
        "            id_werte[spalte] = df[spalte].idxmax()    # Spalte mit dem höchsten Wert\n",
        "    return max_werte, id_werte"
      ],
      "metadata": {
        "id": "ZOJ5WxI4a5V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 1\n",
        "selected_rows = df_Messdaten_drop[df_Messdaten_drop.iloc[:, 4:1305].gt(threshold).any(axis=1)]"
      ],
      "metadata": {
        "id": "pVHO_7FzpGZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_rows"
      ],
      "metadata": {
        "id": "IXtd9p757y9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_eintraege = df_Messdaten_drop[df_Messdaten_drop.isna().any(axis=1)]"
      ],
      "metadata": {
        "id": "m4MGEpHpGJuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_eintraege, df_Messdaten_drop"
      ],
      "metadata": {
        "id": "S7i3GzksIHy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fehlende Spalten einfügen und plotten"
      ],
      "metadata": {
        "id": "BaXvpmLzxMTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate_missing_values(df):\n",
        "\n",
        "    index_start = df_Messdaten_drop.columns.get_loc('290.0')\n",
        "    index_ende = df_Messdaten_drop.columns.get_loc('420.0')\n",
        "    # Interpolieren der fehlenden Werte im DataFrame\n",
        "    # 'limit_direction'='both' ermöglicht die Interpolation in beide Richtungen\n",
        "    # 'limit_area'='inside' sorgt dafür, dass nur innerhalb des existierenden Datenbereichs interpoliert wird\n",
        "    df_interpolated = df.iloc[:, index_start : index_ende].interpolate(method='linear', limit_direction='both', limit_area='inside')\n",
        "    result = pd.concat([df.iloc[:, :4], df_interpolated], axis=1)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "YR9_bk-JnVek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Messdaten_inter = interpolate_missing_values(df_Messdaten_drop)"
      ],
      "metadata": {
        "id": "1ZV8AxiHnWoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_inter_eintraege = df_Messdaten_inter[df_Messdaten_inter.isna().any(axis=1)]\n",
        "nan_inter_eintraege"
      ],
      "metadata": {
        "id": "pHyFNAj3nxPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion um den Maximalen Eintrag einer Spalte zu finden\n",
        "max_werte, id_werte = finde_max_werte_fuer_alle_spalten(df_Messdaten_inter.iloc[:, 4:1306])"
      ],
      "metadata": {
        "id": "6PNKCiOWa_8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schluessel_id = list(id_werte.keys())\n",
        "werte_id = list(id_werte.values())\n",
        "plt.plot(schluessel_id, werte_id)"
      ],
      "metadata": {
        "id": "kgPDlZ2qbIR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotten der maximalen Werte nach Wellenlänge\n",
        "schluessel_max = list(max_werte.keys())\n",
        "werte_max = list(max_werte.values())\n",
        "plt.plot(schluessel_max, werte_max)"
      ],
      "metadata": {
        "id": "jpxT-qAJbH2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dataframe_to_drive(df_Messdaten_inter, '/content/drive/My Drive/Colab_Notebooks/CSV_Messdaten', month)"
      ],
      "metadata": {
        "id": "DO8lF9Vxz6L_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}